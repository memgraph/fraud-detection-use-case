{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3002ab4",
   "metadata": {},
   "source": [
    "# Fraud detection demo\n",
    "\n",
    "The goal of this demo is to showcase how Memgraph can be used to load data from different sources like parquet files stored in external systems like S3 buckets. This is a step-by-step showcase of Memgraph ecosystem. The demo is split into different topics:\n",
    "\n",
    "##### 1.  Loading from AWS S3 bucket using GQLAlchemy\n",
    "AWS S3 is an object storage ideal for large data lakes. It's often used to store large chunks of data in various row-based formats. Our GQLAlchemy enables user to read different formats from such remote data storage, and load it into one of the simplest of Python forms - dictionary.\n",
    "\n",
    "##### 2.  Storing row-based format into Memgraph\n",
    "Such row-based formats are SQL tables, parquet data frames and CSV files. Therefore, these row-based records can be transformed into graph via suitable transformations to nodes and relationships. To accomplish that, GQLAlchemy offers a solution in a form of defining a configuration for loading each table.\n",
    "\n",
    "##### 3.  Exploring the dataset by using GQLAchemy querying\n",
    "With simple queries, information can be gathered by using GQLAlchemy. Fetching these elements enables user to get more insight into different parts of dataset and to create queries which offer a human way of interacting with frauds. For example, gathering all fraud individuals that share an address will lead towards new suspects.\n",
    "\n",
    "##### 4.  Extracting graph features from the dataset\n",
    "Graph algorithms are suitable for use cases such as fraud detection because of the amplification of relationships within the dataset entities. Community detection, PageRank and Node2Vec are just a few of those that we will use to showcase abilities of Memgraph MAGE to extract useful information from graph data. \n",
    "\n",
    "##### 5.  Preparing and running the prediction model based on extracted features\n",
    "Furthermore in the pipeline, once we have extracted features, we can simply run a prediction model on top of them. This enables to adapt to changes and once new data is created, use the model to predict on history information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "928dc543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library needed for graph data handling\n",
    "# GQLAlchemy is the Object Graph Mapper (OGM) - a link between graph database objects and Python objects.\n",
    "import gqlalchemy\n",
    "\n",
    "# Import key features from GQLAlchemy\n",
    "from gqlalchemy import Memgraph, QueryBuilder\n",
    "from gqlalchemy.loaders import S3Translator\n",
    "from gqlalchemy.external_store import store_to_S3\n",
    "\n",
    "# Import environment config and file handlers\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1683742c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, load configuration from environment file\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a19c382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a graph DB connector\n",
    "mg_host = os.getenv(\"MG_HOST\", \"localhost\")\n",
    "mg_port = int(os.getenv(\"MG_PORT\", \"7687\"))\n",
    "db = Memgraph(mg_host, mg_port)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faadc145",
   "metadata": {},
   "source": [
    " ### 1./2.  Loading from AWS S3 bucket using GQLAlchemy &  Storing row-based format into Memgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf7f5633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last thing in setup is loading a AWS S3 configuration\n",
    "DATA_BUCKET = \"insurance-data-bucket/demo\"\n",
    "s3_access_key = os.getenv(\"AWS_ACCESS_KEY_ID\", \"\")\n",
    "s3_secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"\")\n",
    "s3_region = os.getenv(\"AWS_REGION\", \"eu-west-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cccd995",
   "metadata": {},
   "source": [
    "![](img/demo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5cc7c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created index for ADDRESS on add_id\n",
      "Created index for INDIVIDUAL on ind_id\n",
      "Created index for VEHICLE on veh_id\n",
      "Created index for POLICY on pol_id\n",
      "Created index for CLAIM on clm_id\n",
      "Created index for INCIDENT on inc_id\n",
      "Created index for CLAIM_PAYMENT on pay_id\n",
      "Created index for INJURY on inj_id\n",
      "Created trigger INDIVIDUAL__add_id__ADDRESS__add_id\n",
      "Created trigger ADDRESS__add_id__INDIVIDUAL__add_id\n",
      "Created trigger POLICY__insurer_id__INDIVIDUAL__ind_id\n",
      "Created trigger INDIVIDUAL__ind_id__POLICY__insurer_id\n",
      "Created trigger POLICY__insured_with_id__INDIVIDUAL__ind_id\n",
      "Created trigger INDIVIDUAL__ind_id__POLICY__insured_with_id\n",
      "Created trigger POLICY__veh_id__VEHICLE__veh_id\n",
      "Created trigger VEHICLE__veh_id__POLICY__veh_id\n",
      "Created trigger POLICY__add_id__ADDRESS__add_id\n",
      "Created trigger ADDRESS__add_id__POLICY__add_id\n",
      "Created trigger CLAIM__inc_id__INCIDENT__inc_id\n",
      "Created trigger INCIDENT__inc_id__CLAIM__inc_id\n",
      "Created trigger INCIDENT__pol_id__POLICY__pol_id\n",
      "Created trigger POLICY__pol_id__INCIDENT__pol_id\n",
      "Created trigger INCIDENT__add_id__ADDRESS__add_id\n",
      "Created trigger ADDRESS__add_id__INCIDENT__add_id\n",
      "Created trigger CLAIM_PAYMENT__payer_id__INDIVIDUAL__ind_id\n",
      "Created trigger INDIVIDUAL__ind_id__CLAIM_PAYMENT__payer_id\n",
      "Created trigger CLAIM_PAYMENT__payee_id__INDIVIDUAL__ind_id\n",
      "Created trigger INDIVIDUAL__ind_id__CLAIM_PAYMENT__payee_id\n",
      "Created trigger CLAIM_PAYMENT__clm_id__CLAIM__clm_id\n",
      "Created trigger CLAIM__clm_id__CLAIM_PAYMENT__clm_id\n",
      "Created trigger INJURY__clm_id__CLAIM__clm_id\n",
      "Created trigger CLAIM__clm_id__INJURY__clm_id\n",
      "Created trigger INJURY__ind_id__INDIVIDUAL__ind_id\n",
      "Created trigger INDIVIDUAL__ind_id__INJURY__ind_id\n",
      "Loading data from insurance-data-bucket/demo/address.parquet\n",
      "Loading data from insurance-data-bucket/demo/individuals.parquet\n",
      "Loading data from insurance-data-bucket/demo/vehicle.parquet\n",
      "Loading data from insurance-data-bucket/demo/policy.parquet\n",
      "Loading data from insurance-data-bucket/demo/claim.parquet\n",
      "Loading data from insurance-data-bucket/demo/incident.parquet\n",
      "Loading data from insurance-data-bucket/demo/claim_payment.parquet\n",
      "Loading data from insurance-data-bucket/demo/injury.parquet\n",
      "Loading cross table data from insurance-data-bucket/demo/incident_individual.parquet\n"
     ]
    }
   ],
   "source": [
    "# Now, define a mapping between Tables and Nodes/Relationships - nodes are graph entities that are\n",
    "# connected with relationships. We can define them on table-like format and insert them from any\n",
    "# row-based storage such are SQL table, parquet dataframes, CSV files, etc. The only thing neded is \n",
    "# defining mapping between these entities.\n",
    "\n",
    "PATH_TO_CONFIG_YAML = \"./config.yml\"\n",
    "PARQUET_FILE_EXTENSION = \"parquet\"\n",
    "\n",
    "with Path(PATH_TO_CONFIG_YAML).open(\"r\") as f_:\n",
    "    data_configuration = yaml.safe_load(f_)\n",
    "\n",
    "translator = S3Translator(\n",
    "    bucket_name=DATA_BUCKET,\n",
    "    s3_access_key=s3_access_key,\n",
    "    s3_secret_key=s3_secret_key,\n",
    "    s3_region=s3_region,\n",
    "    data_configuration=data_configuration,\n",
    "    file_extension=PARQUET_FILE_EXTENSION,\n",
    ")\n",
    "\n",
    "# Ensure that Memgraph is in clean state once we start out loading\n",
    "translator.translate(drop_database_on_start=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50c32c0",
   "metadata": {},
   "source": [
    "### 3. Exploring the dataset by using GQLAchemy querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8362613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Katherine Richard is insured with POLICY: {valid_from: 2021-08-02, valid_to: 2022-02-20, policy_type: MEDICAL}\n",
      "Katherine Richard is insured with POLICY: {valid_from: 2022-02-08, valid_to: 2023-11-14, policy_type: COLLISION}\n",
      "John Richard is insured with POLICY: {valid_from: 2021-11-14, valid_to: 2022-11-14, policy_type: COMPREHENSIVE}\n",
      "John Richard is insured with POLICY: {valid_from: 2021-05-17, valid_to: 2022-07-18, policy_type: COLLISION}\n",
      "John Richard is insured with POLICY: {valid_from: 2021-10-30, valid_to: 2023-05-08, policy_type: COMPREHENSIVE}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmatak/Programello/fraud-detection-dataset/fraud-detection-demo/gqlalchemy/gqlalchemy/models.py:202: GQLAlchemySubclassNotFoundWarning: ({'INDIVIDUAL'}, <class 'gqlalchemy.models.Node'>)\n",
      "  warnings.warn(GQLAlchemySubclassNotFoundWarning(types, cls))\n",
      "/home/jmatak/Programello/fraud-detection-dataset/fraud-detection-demo/gqlalchemy/gqlalchemy/models.py:202: GQLAlchemySubclassNotFoundWarning: ({'POLICY'}, <class 'gqlalchemy.models.Node'>)\n",
      "  warnings.warn(GQLAlchemySubclassNotFoundWarning(types, cls))\n"
     ]
    }
   ],
   "source": [
    "# Now, when we have everything settled, we are able to explore the data from Memgraph DB.\n",
    "# Querying should be fairly simple, using the GQLAlchemy or openCypher query language.\n",
    "\n",
    "# Here is the example of fetching the policies from certain individual\n",
    "\n",
    "query_builder = QueryBuilder(db)\n",
    "results = list(\n",
    "    query_builder.match() # Match different entities\n",
    "    .node(\"POLICY\", variable=\"policy\") # Match the individual\n",
    "    .to(\"INSURED\") # With relationship INSURED\n",
    "    .node(\"INDIVIDUAL\", variable=\"insurer\") # That has a policy\n",
    "    .return_() # Return all the variables\n",
    "    .limit(5) # Limit the output to 5 elements\n",
    "    .execute()\n",
    ")\n",
    "\n",
    "# Log the output for each of the resulting rows\n",
    "for result in results:\n",
    "    insurer = result[\"insurer\"]\n",
    "    policy = result[\"policy\"]\n",
    "    \n",
    "    print(f\"{insurer.first_name} {insurer.last_name} \"\n",
    "          f\"is insured with POLICY: \"\n",
    "          f\"{{valid_from: {policy.start_date}, valid_to: {policy.end_date}, policy_type: { policy.type}}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c97dc5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isaac Johnson has 5 policies.\n",
      "Chad Curtis has 5 policies.\n",
      "Tyrone Wood has 5 policies.\n",
      "Ryan Williams has 5 policies.\n",
      "Brian Foster has 5 policies.\n"
     ]
    }
   ],
   "source": [
    "# Let's say we are curious about getting the individual with largest number of policies\n",
    "# This time, we will write a raw openCypher query for larger flexibility\n",
    "results = db.execute_and_fetch(\"\"\"\n",
    "    MATCH (insurer:INDIVIDUAL)<-[:INSURED]-(policy:POLICY)\n",
    "    RETURN insurer, count(policy) AS num_policies\n",
    "    ORDER BY num_policies DESC\n",
    "    LIMIT 5;\n",
    "\"\"\")\n",
    "\n",
    "# Log the output for each of the resulting rows\n",
    "for result in results:\n",
    "    insurer = result[\"insurer\"]\n",
    "    num_policies = result[\"num_policies\"]\n",
    "    \n",
    "    print(f\"{insurer.first_name} {insurer.last_name} has {num_policies} policies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fa50e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ann Thomas was involved in claim df92a586 which was marked as fraud.\n",
      "Kim Erickson was involved in claim 4cf3f0a3 which was marked as fraud.\n",
      "Lauren Perry was involved in claim 280e00ff which was marked as fraud.\n",
      "Stephanie Johnson was involved in claim 155d46b4 which was marked as fraud.\n",
      "Barbara Pitts was involved in claim 8ba09cef which was marked as fraud.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmatak/Programello/fraud-detection-dataset/fraud-detection-demo/gqlalchemy/gqlalchemy/models.py:202: GQLAlchemySubclassNotFoundWarning: ({'CLAIM'}, <class 'gqlalchemy.models.Node'>)\n",
      "  warnings.warn(GQLAlchemySubclassNotFoundWarning(types, cls))\n"
     ]
    }
   ],
   "source": [
    "# Moving into interest area of potential frauds\n",
    "\n",
    "# List some fraudulent claims and their associates\n",
    "results = db.execute_and_fetch(\"\"\"\n",
    "    MATCH (individual:INDIVIDUAL)<-[:INSURED]-(policy:POLICY)<-[]-(incident:INCIDENT)<-[]-(fraud:CLAIM {fraud: True})\n",
    "    RETURN individual, fraud\n",
    "    LIMIT 5;\n",
    "\"\"\")\n",
    "\n",
    "# Log the output for each of the resulting rows\n",
    "for result in results:\n",
    "    insurer = result[\"individual\"]\n",
    "    fraudulent_claim = result[\"fraud\"]\n",
    "    \n",
    "    print(f\"{insurer.first_name} {insurer.last_name} was involved in claim {fraudulent_claim.clm_id} which was marked as fraud.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05d68a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After obtaining frauds in history, we would like to extract features for each of them, and encorporate\n",
    "# them to detect the new fraud. \n",
    "\n",
    "# First, we need to simulate out feature store. For these purposes, we'll use Pandas, frequent DataFrame format\n",
    "# Let's define some features\n",
    "import pandas as pd\n",
    "class FeatureStore:\n",
    "    IND_COUNT = \"ind_count\"\n",
    "    AMT_PAID = \"amount_paid\"\n",
    "    POL_EXPIRED = \"policy_expired\"\n",
    "    POL_PREMIUM = \"policy_premium\"\n",
    "    INFLUENCE = \"influence\"\n",
    "    NUM_FRAUDS_NEIGHBORHOOD = \"num_frauds_neighborhood\"\n",
    "    NUM_FRAUDS_COMMUNITY = \"num_frauds_community\"\n",
    "    EMBEDDING = \"embedding\"\n",
    "    FRAUD = 'fraud'\n",
    "    \n",
    "    \n",
    "FEATURE_NAMES = [\n",
    "    FeatureStore.IND_COUNT, \n",
    "    FeatureStore.AMT_PAID, \n",
    "    FeatureStore.POL_EXPIRED, \n",
    "    FeatureStore.POL_PREMIUM, \n",
    "    FeatureStore.INFLUENCE, \n",
    "    FeatureStore.NUM_FRAUDS_NEIGHBORHOOD, \n",
    "    FeatureStore.NUM_FRAUDS_COMMUNITY, \n",
    "    FeatureStore.EMBEDDING, \n",
    "    FeatureStore.FRAUD, \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaf53c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of individuals involved:  3\n",
      "Total amount paid:  4035.4\n",
      "Is policy expired:  True\n",
      "Policy premium type:  A\n"
     ]
    }
   ],
   "source": [
    "# First, define simple feature types\n",
    "from datetime import datetime, date\n",
    "\n",
    "# Count of individuals related to the incident/claim could be\n",
    "# useful information when combined with other features.\n",
    "def get_individual_count(claim_id: str) -> int:\n",
    "    return next(db.execute_and_fetch(f\"\"\"\n",
    "        MATCH (claim:CLAIM {{clm_id: '{claim_id}'}})-[]->(incident:INCIDENT)-[]->(individual:INDIVIDUAL)\n",
    "        RETURN COUNT(individual) AS {FeatureStore.IND_COUNT}\n",
    "    \"\"\"))[FeatureStore.IND_COUNT]\n",
    "\n",
    "\n",
    "# Price paid for the claim can be a fine indicator of whether there\n",
    "# is a suspicious activity going on.\n",
    "def get_amount_paid(claim_id: str) -> float:\n",
    "    amt_paid =  next(db.execute_and_fetch(f\"\"\"\n",
    "        MATCH (claim:CLAIM {{clm_id: '{claim_id}'}})<-[]-(payment:CLAIM_PAYMENT)\n",
    "        RETURN SUM(payment.amount) AS {FeatureStore.AMT_PAID}\n",
    "    \"\"\"))[FeatureStore.AMT_PAID]\n",
    "    \n",
    "    return amt_paid if amt_paid else 0\n",
    "\n",
    "\n",
    "# Filing a claim on expired policy might be a bad sign\n",
    "def get_policy_expired(claim_id: str) -> bool:\n",
    "    today = str(datetime.now().date())\n",
    "    return next(db.execute_and_fetch(f\"\"\"\n",
    "        MATCH (claim:CLAIM {{clm_id: '{claim_id}'}})-[]->(:INCIDENT)-[]->(policy:POLICY)\n",
    "        RETURN  policy.end_date > '{today}' AS {FeatureStore.POL_EXPIRED}\n",
    "    \"\"\"))[FeatureStore.POL_EXPIRED]\n",
    "\n",
    "\n",
    "# Depending on premium level, the amount covered can vary, suspicion too.\n",
    "def get_policy_premium(claim_id: str) -> bool:\n",
    "    premium = next(db.execute_and_fetch(f\"\"\"\n",
    "        MATCH (claim:CLAIM {{clm_id: '{claim_id}'}})-[]->(:INCIDENT)-[]->(policy:POLICY)\n",
    "        RETURN  policy.premium AS {FeatureStore.POL_PREMIUM} \n",
    "    \"\"\"))[FeatureStore.POL_PREMIUM]\n",
    "    return premium if premium else \"U\"\n",
    "\n",
    "\n",
    "print(\"Number of individuals involved: \", get_individual_count('a6a1edd5'))\n",
    "print(\"Total amount paid: \", get_amount_paid('a6a1edd5'))\n",
    "print(\"Is policy expired: \", get_policy_expired('a6a1edd5'))\n",
    "print(\"Policy premium type: \", get_policy_premium('a6a1edd5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867f46af",
   "metadata": {},
   "source": [
    "### 4.  Extracting graph features from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fa28581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's time to apply some graph analytics on our fraud model\n",
    "\n",
    "# After splitting the graph entities into communities, communities with individuals\n",
    "# related to previous frauds will have larger probability of being fraudulent\n",
    "def run_community_detection() -> None:\n",
    "    db.execute(f\"\"\"\n",
    "        CALL community_detection_online.set() YIELD node, community_id\n",
    "        SET node.community = community_id;\n",
    "    \"\"\")\n",
    "\n",
    "# Measure of centrality can help \n",
    "def run_pagerank() -> None:\n",
    "    db.execute(f\"\"\"\n",
    "        CALL pagerank_online.set(100, 0.2) YIELD node, rank\n",
    "        SET node.influence = rank;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "# These are highly artificial vectors of numbers, but they encode a graph structure.\n",
    "# Malicious groups act alike, therefore their structural encodings will be the same.\n",
    "def run_node_embedding() -> None:\n",
    "    db.execute(f\"\"\"\n",
    "        CALL node2vec.set_embeddings() YIELD *;\n",
    "    \"\"\")\n",
    "\n",
    "run_community_detection()\n",
    "run_pagerank()\n",
    "run_node_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e28e97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frauds in community:  1\n",
      "Number of close/related frauds:  0\n",
      "Claim influence:  7.415084546114143e-05\n",
      "\n",
      "Claim vector embedding [-0.0036331575829535723, 0.16035115718841553, 0.06788065284490585, 0.026274941861629486, -0.040417518466711044, -0.047820936888456345, -0.07806451618671417, 0.11748766154050827, 0.0007394145941361785, -0.04617908596992493, -0.09449166804552078, -0.1764097809791565, -0.14741159975528717, -0.12504450976848602, 0.03886666148900986, -0.058797337114810944, -0.0891508087515831, -0.16438713669776917, 0.0643087849020958, -0.011099050752818584, 0.01349132414907217, -0.03189538046717644, 0.07672545313835144, -0.01702423021197319, 0.1600838601589203, -0.00922150444239378, 0.07333815097808838, 0.07401634752750397, -0.12490852177143097, -0.10909868776798248, 0.01721559278666973, 0.02600823901593685, -0.09598367661237717, -0.033101506531238556, 0.14138612151145935, -0.1140926256775856, 0.12498851120471954, -0.038432780653238297, 0.039282139390707016, -0.12144436687231064, -0.05945511534810066, 0.01725493185222149, -0.05777638033032417, 0.01720389910042286, -0.039024826139211655, -0.16154387593269348, -0.14959114789962769, 0.2005007416009903, -0.003744284389540553, 0.14539627730846405, -0.08271140605211258, -0.100226029753685, -0.2073819637298584, -0.07729724049568176, -0.1978376805782318, 0.10744943469762802, 0.10329942405223846, 0.06661120057106018, -0.03255551680922508, -0.0058098407462239265, -0.05432601645588875, -0.09475209563970566, -0.3923206925392151, 0.12528491020202637, 0.11599321663379669, -0.04142870754003525, -0.09659869968891144, -0.048520009964704514, 0.06362593173980713, 0.1061624139547348, 0.020255988463759422, 0.08081728219985962, 0.15399236977100372, -0.08409878611564636, 0.020579975098371506, -0.026234105229377747, -0.10126858204603195, -0.0452658049762249, 0.006508437916636467, 0.044537827372550964, -0.0030844099819660187, -0.11405310779809952, -0.050888173282146454, 0.03709057718515396, -0.0508611835539341, -0.057191282510757446, -0.04480256140232086, 0.19547195732593536, 0.015976080670952797, -0.014811174012720585, 0.04178924486041069, -0.07583396881818771, 0.1485559046268463, -0.009197995066642761, 0.11174754798412323, -0.020998263731598854, 0.08333038538694382, -0.05143016576766968, -0.11641840636730194, 0.0744328498840332]\n"
     ]
    }
   ],
   "source": [
    "def get_num_frauds_community(claim_id: str) -> float:\n",
    "    return next(db.execute_and_fetch(f\"\"\"\n",
    "        MATCH (claim:CLAIM {{clm_id: '{claim_id}'}}), (fraud:CLAIM {{fraud: True}})\n",
    "        WHERE claim.community = fraud.community\n",
    "        RETURN COUNT(fraud) AS {FeatureStore.NUM_FRAUDS_COMMUNITY}\n",
    "    \"\"\"))[FeatureStore.NUM_FRAUDS_COMMUNITY]\n",
    "\n",
    "def get_num_frauds_neighborhood(claim_id: str) -> float:\n",
    "    return next(db.execute_and_fetch(f\"\"\"\n",
    "        MATCH (claim:CLAIM {{clm_id: '{claim_id}'}})-[*bfs ..4]-(fraud:CLAIM {{fraud: True}})\n",
    "        RETURN COUNT(fraud) AS {FeatureStore.NUM_FRAUDS_NEIGHBORHOOD}\n",
    "    \"\"\"))[FeatureStore.NUM_FRAUDS_NEIGHBORHOOD]\n",
    "\n",
    "def get_influence(claim_id: str) -> float:\n",
    "    return next(db.execute_and_fetch(f\"\"\"\n",
    "        MATCH (claim:CLAIM {{clm_id: '{claim_id}'}})\n",
    "        RETURN claim.influence AS {FeatureStore.INFLUENCE}\n",
    "    \"\"\"))[FeatureStore.INFLUENCE]\n",
    "\n",
    "def get_embedding(claim_id: str) -> float:\n",
    "    return next(db.execute_and_fetch(f\"\"\"\n",
    "        MATCH (claim:CLAIM {{clm_id: '{claim_id}'}})\n",
    "        RETURN claim.embedding AS {FeatureStore.EMBEDDING}\n",
    "    \"\"\"))[FeatureStore.EMBEDDING]\n",
    "\n",
    "print(\"Number of frauds in community: \", get_num_frauds_community('a6a1edd5'))\n",
    "print(\"Number of close/related frauds: \", get_num_frauds_neighborhood('a6a1edd5'))\n",
    "print(\"Claim influence: \", get_influence('a6a1edd5'))\n",
    "print()\n",
    "print(\"Claim vector embedding\", get_embedding('a6a1edd5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffeab41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmatak/Programello/fraud-detection-dataset/fraud-detection-demo/gqlalchemy/gqlalchemy/models.py:202: GQLAlchemySubclassNotFoundWarning: ({'CLAIM'}, <class 'gqlalchemy.models.Node'>)\n",
      "  warnings.warn(GQLAlchemySubclassNotFoundWarning(types, cls))\n"
     ]
    }
   ],
   "source": [
    "# Generate all features for each possible claim\n",
    "import pandas as pd\n",
    "\n",
    "def get_claims():\n",
    "    return db.execute_and_fetch(f\"\"\"\n",
    "        MATCH (claim:CLAIM)\n",
    "        RETURN claim\n",
    "    \"\"\")\n",
    "\n",
    "CLAIM_ID = 'claim_id'\n",
    "feature_store = pd.DataFrame(columns = FEATURE_NAMES + [CLAIM_ID])\n",
    "\n",
    "\n",
    "# Store the claims into feature store - a dataframe\n",
    "for i, result in enumerate(get_claims()):\n",
    "    claim = result['claim']\n",
    "    claim_features = {\n",
    "        FeatureStore.IND_COUNT: get_individual_count(claim.clm_id),\n",
    "        FeatureStore.AMT_PAID: get_amount_paid(claim.clm_id),\n",
    "        FeatureStore.POL_EXPIRED: get_policy_expired(claim.clm_id),\n",
    "        FeatureStore.POL_PREMIUM: get_policy_premium(claim.clm_id),\n",
    "        FeatureStore.NUM_FRAUDS_COMMUNITY: get_num_frauds_community(claim.clm_id),\n",
    "        FeatureStore.NUM_FRAUDS_NEIGHBORHOOD: get_num_frauds_neighborhood(claim.clm_id),\n",
    "        FeatureStore.INFLUENCE: get_influence(claim.clm_id),\n",
    "        FeatureStore.EMBEDDING: [get_embedding(claim.clm_id)],\n",
    "        FeatureStore.FRAUD: claim.fraud,\n",
    "        CLAIM_ID: claim.clm_id\n",
    "    }\n",
    "    feature_store = pd.concat([feature_store, pd.DataFrame(claim_features)], ignore_index=True)\n",
    "\n",
    "feature_store = feature_store.set_index(CLAIM_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c3e6f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data into AWS S3 storage\n",
    "\n",
    "import pyarrow as pa\n",
    "\n",
    "store_to_S3(pa.Table.from_pandas(feature_store),\n",
    "            f\"{DATA_BUCKET}/feature-store\",\n",
    "            \"features_\" + str(datetime.now()),\n",
    "            region=s3_region,\n",
    "            access_key=s3_access_key,\n",
    "            secret_key=s3_secret_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef3619e",
   "metadata": {},
   "source": [
    "### 5.  Preparing and running the prediction model based on extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21d9fb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmatak/Programello/fraud-detection-dataset/fraud-detection-demo/.venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Predicted', ylabel='Actual'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import resample\n",
    "from sklearn import metrics\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Extracting the features for training phase out of feature store\n",
    "X = feature_store[[\n",
    "    FeatureStore.IND_COUNT, \n",
    "    FeatureStore.AMT_PAID, \n",
    "    FeatureStore.NUM_FRAUDS_COMMUNITY,\n",
    "    FeatureStore.POL_PREMIUM,\n",
    "    FeatureStore.NUM_FRAUDS_NEIGHBORHOOD,\n",
    "    FeatureStore.INFLUENCE,\n",
    "]]\n",
    "# Selecting the feature type. \n",
    "X = X.astype({\n",
    "    FeatureStore.IND_COUNT: int,\n",
    "    FeatureStore.AMT_PAID: float,\n",
    "    FeatureStore.NUM_FRAUDS_COMMUNITY: int,\n",
    "    FeatureStore.POL_PREMIUM: \"category\",\n",
    "    FeatureStore.NUM_FRAUDS_NEIGHBORHOOD: int,\n",
    "    FeatureStore.INFLUENCE: float,\n",
    "})\n",
    "\n",
    "# If feature is categorical, encode them in one-hot encoding scheme\n",
    "dummies = pd.get_dummies(X[FeatureStore.POL_PREMIUM], prefix=FeatureStore.POL_PREMIUM)\n",
    "X = X.join(dummies)\n",
    "X = X.drop(columns=[FeatureStore.POL_PREMIUM])\n",
    "y = feature_store[FeatureStore.FRAUD]\n",
    "y = y.astype({\n",
    "    FeatureStore.FRAUD: bool,\n",
    "})\n",
    "feature_columns = X.columns\n",
    "\n",
    "# Split the dataset into training and testing\n",
    "# NOTE: In real-life scenario, this split will be time-dependent\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=0)\n",
    "\n",
    "# Upsample the minority class data\n",
    "train_data = pd.concat((X_train, y_train), axis=1)\n",
    "X_positive = train_data[train_data[FeatureStore.FRAUD] == True]\n",
    "X_negative = train_data[train_data[FeatureStore.FRAUD] == False]\n",
    "\n",
    "X_positive = resample(X_positive, \n",
    "                      replace=True,    \n",
    "                      n_samples=len(X_negative) // 2)\n",
    "train_data = pd.concat((X_positive, X_negative), axis=0)\n",
    "\n",
    "y_train = train_data[FeatureStore.FRAUD]\n",
    "X_train = train_data.drop(columns=[FeatureStore.FRAUD])\n",
    "\n",
    "# Fit the simplest logistic regression model by using scikit-learn \n",
    "logistic_regression= LogisticRegression()\n",
    "logistic_regression.fit(X_train,y_train)\n",
    "y_pred=logistic_regression.predict(X_test)\n",
    "\n",
    "# Draw a confusion matrix and discuss the results. Here, the results are\n",
    "# correlated to the dataset which is not highly accurate.\n",
    "confusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "sn.heatmap(\n",
    "    confusion_matrix, \n",
    "    annot=True,\n",
    "    fmt='g',\n",
    "    cmap='Blues'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e8b9108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To enable faster processing, Memgraph offers a dynamic analytics. Once data is inserted, analytics\n",
    "# is calculated on the fly. To set it, we create triggers.\n",
    "\n",
    "def create_community_detection_trigger():\n",
    "    db.execute(\"\"\"\n",
    "        CREATE TRIGGER community_detection_online_trigger BEFORE COMMIT\n",
    "        EXECUTE CALL community_detection_online.update(createdVertices, createdEdges, updatedVertices, updatedEdges, deletedVertices, deletedEdges) YIELD node, community_id\n",
    "        SET node.community = community_id;\n",
    "    \"\"\")\n",
    "\n",
    "def create_influence_trigger():\n",
    "    db.execute(\"\"\"\n",
    "        CREATE TRIGGER influence_trigger BEFORE COMMIT\n",
    "        EXECUTE CALL pagerank_online.update(createdVertices, createdEdges, deletedVertices, deletedEdges) YIELD *\n",
    "        SET node.influence = rank;\n",
    "    \"\"\")\n",
    "    \n",
    "create_community_detection_trigger()\n",
    "create_influence_trigger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93420c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's create another item in the database\n",
    "# We'll simulate how items are inserted into SQL DB\n",
    "\n",
    "NEW_CLAIM_ID = 'bc7462cb'\n",
    "\n",
    "INCIDENT_ID = 'bf418431'\n",
    "ADDRESS_ID = '4700feb8'\n",
    "POLICY_ID = 'b74bf655'\n",
    "CLAIM2_ID = 'e93ffa1a'\n",
    "INJURY_ID = 'abde1883'\n",
    "INDIVIDUAL_ID = '7324cb18'\n",
    "PAYMENT_ID = 'bd384ce3'\n",
    "PAYEE_ID = '35d27b82'\n",
    "\n",
    "# Create INCIDENT in table\n",
    "def create_incident(incident_id, policy_id, address_id):\n",
    "    date = str(datetime.now())\n",
    "    db.execute(f\"\"\"\n",
    "    CREATE (:INCIDENT {{\n",
    "        accident_date: \"{date}\",\n",
    "        add_id: \"{address_id}\",\n",
    "        file_date: \"{date}\",\n",
    "        inc_id: \"{incident_id}\",\n",
    "        pol_id: \"{policy_id}\" \n",
    "    }});\n",
    "    \"\"\")\n",
    "\n",
    "def create_incident_individual(incident_id, individual_id):\n",
    "    db.execute(f\"\"\"\n",
    "    MATCH (a:INCIDENT {{inc_id: \"{incident_id}\"}}) \n",
    "    MATCH (b:INDIVIDUAL {{ind_id: \"{individual_id}\"}})\n",
    "    CREATE (a)-[:INCIDENT]->(b);\n",
    "    \"\"\")\n",
    "\n",
    "# Create claims\n",
    "def create_claim(claim_id, incident_id, amount):\n",
    "    db.execute(f\"\"\"\n",
    "    CREATE (:CLAIM {{\n",
    "            clm_id: \"{claim_id}\",\n",
    "            inc_id: \"{incident_id}\",\n",
    "            amount: \"{amount}\"\n",
    "    }});\n",
    "    \"\"\")\n",
    "\n",
    "# Create injuries\n",
    "def create_injury(injury_id, injury_type, claim_id, individual_id):\n",
    "    db.execute(f\"\"\"\n",
    "    CREATE (:INJURY {{\n",
    "            clm_id: \"{claim_id}\",\n",
    "            ind_id: \"\",\n",
    "            inj_id: \"{injury_id}\",\n",
    "            type: \"{injury_type}\"\n",
    "    }});\n",
    "    \"\"\")\n",
    "\n",
    "# Create payment\n",
    "def create_payment(payment_id, amount, claim_id, payer_id, payee_id):\n",
    "    db.execute(f\"\"\"\n",
    "    CREATE (:CLAIM_PAYMENT {{\n",
    "        amount: {amount},\n",
    "        clm_id: \"{claim_id}\",\n",
    "        pay_id: \"{payment_id}\",\n",
    "        payee_id: \"{payee_id}\",\n",
    "        payer_id: \"{payer_id}\"\n",
    "    }});\n",
    "    \"\"\")\n",
    "\n",
    "create_incident(INCIDENT_ID, POLICY_ID, ADDRESS_ID)\n",
    "create_incident_individual(INCIDENT_ID, INDIVIDUAL_ID)\n",
    "create_incident_individual(INCIDENT_ID, PAYEE_ID)\n",
    "\n",
    "create_claim(NEW_CLAIM_ID, INCIDENT_ID, 4313)\n",
    "create_claim(CLAIM2_ID, INCIDENT_ID, 2500)\n",
    "create_injury(INJURY_ID, \"Legs\", NEW_CLAIM_ID, INDIVIDUAL_ID)\n",
    "create_payment(PAYMENT_ID, 4300, NEW_CLAIM_ID, INDIVIDUAL_ID, PAYEE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bd644c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim bc7462cb fraud prediction: [False]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ind_count</th>\n",
       "      <th>amount_paid</th>\n",
       "      <th>num_frauds_community</th>\n",
       "      <th>num_frauds_neighborhood</th>\n",
       "      <th>influence</th>\n",
       "      <th>policy_premium_A</th>\n",
       "      <th>policy_premium_B</th>\n",
       "      <th>policy_premium_C</th>\n",
       "      <th>policy_premium_D</th>\n",
       "      <th>policy_premium_U</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bc7462cb</th>\n",
       "      <td>2</td>\n",
       "      <td>4300.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ind_count  amount_paid  num_frauds_community  \\\n",
       "bc7462cb          2       4300.0                     0   \n",
       "\n",
       "          num_frauds_neighborhood  influence  policy_premium_A  \\\n",
       "bc7462cb                        1   0.000133                 1   \n",
       "\n",
       "          policy_premium_B  policy_premium_C  policy_premium_D  \\\n",
       "bc7462cb                 0                 0                 0   \n",
       "\n",
       "          policy_premium_U  \n",
       "bc7462cb                 0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_features(clm_id, columns):\n",
    "    X = pd.DataFrame({\n",
    "        FeatureStore.IND_COUNT: get_individual_count(clm_id),\n",
    "        FeatureStore.AMT_PAID: get_amount_paid(clm_id),\n",
    "        FeatureStore.POL_EXPIRED: get_policy_expired(clm_id),\n",
    "        FeatureStore.POL_PREMIUM: get_policy_premium(clm_id),\n",
    "        FeatureStore.NUM_FRAUDS_COMMUNITY: get_num_frauds_community(clm_id),\n",
    "        FeatureStore.NUM_FRAUDS_NEIGHBORHOOD: get_num_frauds_neighborhood(clm_id),\n",
    "        FeatureStore.INFLUENCE: get_influence(clm_id),\n",
    "    }, index=[clm_id])\n",
    "    # Selecting the feature type. \n",
    "    X = X.astype({\n",
    "        FeatureStore.IND_COUNT: int,\n",
    "        FeatureStore.AMT_PAID: float,\n",
    "        FeatureStore.NUM_FRAUDS_COMMUNITY: int,\n",
    "        FeatureStore.POL_PREMIUM: \"category\",\n",
    "        FeatureStore.NUM_FRAUDS_NEIGHBORHOOD: int,\n",
    "        FeatureStore.INFLUENCE: float,\n",
    "    })\n",
    "    X = pd.get_dummies(X)\n",
    "    X = X.reindex(columns = columns, fill_value=0)\n",
    "    return X\n",
    "\n",
    "new_features = extract_features(NEW_CLAIM_ID, feature_columns)\n",
    "y_pred=logistic_regression.predict(new_features)\n",
    "print(f\"Claim {NEW_CLAIM_ID} fraud prediction: {y_pred}\")\n",
    "new_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
